# feature_importance
An exploration of various feature importance and selection techniques.

Feature importances are useful tools to interpret machine learning models. Here, I explore some of the widely used techniques like:

1. Spearman's rank correlation coefficient
2. Principle Component Analysis
3. Minimal Redundancy Maximal Relevance (mRMR)
4. Permutation Importance
5. Drop Column Importance
6. Shap Importance

I have also explored methods that help compare performance of these techniques and develop a better understanding of feature importance in general:

1. Comparing feature importance techniques - Top N feature loss
2. Automatic Feature Selection
3. Variance in Feature Importances
4. Empirical P-Values

Thanks to Prof. [Terence Parr](https://github.com/parrt) for his guidance and support in this school project.

