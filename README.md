# feature_importance
An exploration of various feature importance and selection techniques.

Feature importances are useful tools to interpret machine learning models. Here, we explore some of the widely used techniques like:

1. Spearman's rank correlation coefficient
2. Principle Component Analysis
3. Minimal Redundancy Maximal Relevance (mRMR)
4. Permutation Importance
5. Drop Column Importance
6. Shap Importance

We then initiate a discussion on the efficiency of these techniques and explore methods to enhance our understanding.

1. Comparing feature importance techniques - Top N feature loss
2. Automatic Feature Selection
3. Variance in Feature Importances
4. Empirical P-Values

Thanks to Prof. [Terence Parr](https://github.com/parrt) for his guidance and support in this school project.

